name: GPU CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  gpu-tests:
    runs-on: [self-hosted, gpu, linux]

    steps:
    - uses: actions/checkout@v4

    - name: Set up environment
      run: |
        cp .env.base .env
        echo "WANDB_API_KEY=${{ secrets.WANDB_API_KEY }}" >> .env
        echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .env

    - name: Build GPU Docker image
      run: |
        make build-gpu

    - name: Test GPU availability
      run: |
        docker-compose -f docker-compose.yml run --rm textforensics-dev \
          python -c "
          import torch
          assert torch.cuda.is_available(), 'CUDA not available'
          print(f'✅ GPU: {torch.cuda.get_device_name(0)}')
          print(f'✅ CUDA Version: {torch.version.cuda}')
          print(f'✅ PyTorch Version: {torch.__version__}')
          print(f'✅ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
          "

    - name: Run GPU benchmarks
      run: |
        docker-compose -f docker-compose.yml run --rm textforensics-dev \
          python scripts/benchmark_gpu.py

    - name: Run unit tests
      run: |
        docker-compose -f docker-compose.yml run --rm textforensics-dev \
          pytest tests/unit/ -v --tb=short

    - name: Quick training test
      run: |
        docker-compose -f docker-compose.yml run --rm textforensics-dev \
          python scripts/train.py +experiment=quick_test training.max_epochs=1

    - name: Code quality checks
      run: |
        docker-compose -f docker-compose.yml run --rm textforensics-dev \
          bash -c "
          black --check src/ tests/ &&
          ruff check src/ tests/ &&
          mypy src/
          "

    - name: Cleanup
      if: always()
      run: |
        docker-compose -f docker-compose.yml down
        docker system prune -f
