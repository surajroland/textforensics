services:
  # Main development service
  textforensics-dev:
    image: ${DOCKER_REGISTRY:-localhost}/textforensics:${VERSION:-dev}
    container_name: textforensics-dev
    user: "${UID:-1000}:${GID:-1000}"
    build:
      context: .
      dockerfile: Dockerfile
      target: development  # Multi-stage build target
      args:
        - BUILDKIT_INLINE_CACHE=1
        - GIT_USER_NAME=${GIT_USER_NAME}
        - GIT_USER_EMAIL=${GIT_USER_EMAIL}
        - SETUP_DEV_CONFIG=true
        - USER_ID=${UID:-1000}
        - GROUP_ID=${GID:-1000}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NODE_ENV=development
      - CUDA_VISIBLE_DEVICES=0
      - WANDB_API_KEY=${WANDB_API_KEY}
      - HF_TOKEN=${HF_TOKEN}
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - NVIDIA_VISIBLE_DEVICES=all
      # RTX 5070 Ti optimizations
      - TORCH_CUDA_ARCH_LIST=8.9+PTX
      - CUDA_LAUNCH_BLOCKING=0
      - TORCH_CUDNN_V8_API_ENABLED=1
    volumes:
      - .:/workspace
      - ${HOME}/.gitconfig:/home/appuser/.gitconfig:ro
      - ${HOME}/.ssh:/home/appuser/.ssh:rw
      - nvidia_cache:/home/appuser/.cache/nvidia
      - huggingface_cache:/home/appuser/.cache/huggingface
      - wandb_cache:/home/appuser/.cache/wandb
      - jupyter_data:/home/appuser/.jupyter
      - bash_history:/home/appuser/.bash_history
      - /var/run/docker.sock:/var/run/docker.sock  # Docker-in-Docker
    ports:
      - "${JUPYTER_PORT:-8888}:8888"
      - "${TENSORBOARD_PORT:-6006}:6006"
      - "${WANDB_PORT:-8097}:8097"
      - "${API_PORT:-8000}:8000"
      - "${GRADIO_PORT:-7860}:7860"
    networks:
      - textforensics-network
    restart: unless-stopped
    stdin_open: true
    tty: true
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; assert torch.cuda.is_available()"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: >
      bash -c "
        echo 'üöÄ Starting TextForensics Development Environment'
        echo 'üíª Hardware: Ryzen 9 9950X3D + RTX 5070 Ti'
        echo 'üê≥ Container: ${DOCKER_REGISTRY:-localhost}/textforensics:${VERSION:-dev}'
        echo 'üêö Shell: Bash with Starship prompt'
        echo ''
        echo 'üìä Jupyter Lab: http://localhost:${JUPYTER_PORT:-8888}'
        echo 'üìà TensorBoard: http://localhost:${TENSORBOARD_PORT:-6006}'
        echo 'üîç Wandb: http://localhost:${WANDB_PORT:-8097}'
        echo ''
        jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root &
        tensorboard --logdir=outputs/logs --host=0.0.0.0 --port=6006 &
        exec bash
      "

  # Training service for production workloads
  textforensics-trainer:
    image: ${DOCKER_REGISTRY:-localhost}/textforensics:${VERSION:-dev}
    container_name: textforensics-trainer
    extends: textforensics-dev
    environment:
      - NODE_ENV=production
      - CUDA_VISIBLE_DEVICES=all
      - NCCL_DEBUG=INFO
      - TORCH_DISTRIBUTED_DEBUG=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: python scripts/training/train.py
    profiles: ["training"]  # Only start with --profile training

  # API service for inference
  textforensics-api:
    image: ${DOCKER_REGISTRY:-localhost}/textforensics:${VERSION:-dev}
    container_name: textforensics-api
    extends: textforensics-dev
    environment:
      - NODE_ENV=production
    ports:
      - "${API_PORT:-8000}:8000"
    command: uvicorn textforensics.api:app --host 0.0.0.0 --port 8000 --reload
    profiles: ["api"]  # Only start with --profile api
    depends_on:
      - textforensics-dev

  # Jupyter-only service for research
  textforensics-research:
    image: ${DOCKER_REGISTRY:-localhost}/textforensics:${VERSION:-dev}
    container_name: textforensics-research
    extends: textforensics-dev
    ports:
      - "${JUPYTER_PORT:-8888}:8888"
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
    profiles: ["research"]  # Only start with --profile research

  # Database for storing results (optional)
  textforensics-db:
    image: postgres:15-alpine
    container_name: textforensics-db
    environment:
      - POSTGRES_DB=textforensics
      - POSTGRES_USER=${DB_USER:-textforensics}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-textforensics123}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "${DB_PORT:-5432}:5432"
    networks:
      - textforensics-network
    profiles: ["database"]  # Only start with --profile database

  # Redis for caching (optional)
  textforensics-redis:
    image: redis:7-alpine
    container_name: textforensics-redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - textforensics-network
    profiles: ["cache"]  # Only start with --profile cache

# Named volumes for better data management
volumes:
  nvidia_cache:
    name: textforensics_nvidia_cache
  huggingface_cache:
    name: textforensics_huggingface_cache
  wandb_cache:
    name: textforensics_wandb_cache
  jupyter_data:
    name: textforensics_jupyter_data
  bash_history:
    name: textforensics_bash_history
  postgres_data:
    name: textforensics_postgres_data
  redis_data:
    name: textforensics_redis_data

# Custom network for service communication
networks:
  textforensics-network:
    name: textforensics-network
    driver: bridge
