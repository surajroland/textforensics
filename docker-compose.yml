# TextForensics Docker Compose Configuration
# Multi-service setup for development, training, research, and API deployment
# Uses environment variables from .env for configuration

services:
  # ==============================================================================
  # DEVELOPMENT SERVICE - Primary development environment
  # ==============================================================================
  textforensics-dev:
    build:
      context: .                    # Build context is current directory
      target: development           # Multi-stage build target
    image: textforensics-dev:${VERSION}
    container_name: textforensics-dev

    # Volume mounts for development workflow
    volumes:
      - .:/workspace                              # Mount project directory
      - ~/.ssh:/root/.ssh:ro                      # SSH keys (read-only)
      - ~/.gitconfig:/root/.gitconfig:ro          # Git configuration
      - bash-history:/root                        # Persist bash history
      - textforensics-data:/workspace/data        # Project data
      - textforensics-logs:/workspace/logs        # Application logs
      - textforensics-models:/workspace/models    # ML models
      - textforensics-cache:/root/.cache          # Cache directory
      - vscode-server:/root/.vscode-server        # VS Code server files
      - /var/run/docker.sock:/var/run/docker.sock # Docker-in-Docker

    # Port mappings for development services
    ports:
      - "${JUPYTER_PORT:-8888}:8888"      # Jupyter Lab
      - "${TENSORBOARD_PORT:-6006}:6006"  # TensorBoard
      - "${WANDB_PORT:-8097}:8097"        # Weights & Biases
      - "${API_PORT:-8000}:8000"          # FastAPI
      - "${GRADIO_PORT:-7860}:7860"       # Gradio UI

    # Environment variables for development
    environment:
      - NODE_ENV=development               # Node.js environment
      - PYTHONPATH=/workspace/src          # Python module path
      - HYDRA_FULL_ERROR=1                # Hydra debug mode
      - GITHUB_TOKEN=${GITHUB_TOKEN}      # GitHub API access
      - HF_TOKEN=${HF_TOKEN}              # Hugging Face API
      - WANDB_API_KEY=${WANDB_API_KEY}    # Weights & Biases API

    # Startup command with service initialization
    command: >
      bash -c "
        echo 'üöÄ Starting TextForensics Development Environment'
        echo 'üêö Shell: Bash with Starship prompt'
        echo ''
        echo 'üìä Jupyter Lab: http://localhost:${JUPYTER_PORT:-8888}'
        echo 'üìà TensorBoard: http://localhost:${TENSORBOARD_PORT:-6006}'
        echo 'üîç Wandb: http://localhost:${WANDB_PORT:-8097}'
        echo ''
        jupyter lab --ip=0.0.0.0 --port=${JUPYTER_PORT:-8888} --no-browser --allow-root &
        tensorboard --logdir=outputs/logs --host=0.0.0.0 --port=${TENSORBOARD_PORT:-6006} &
        exec bash
      "

    # Container configuration
    stdin_open: true                     # Keep STDIN open
    tty: true                           # Allocate pseudo-TTY
    restart: unless-stopped             # Restart policy

    # Health monitoring
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; assert torch.cuda.is_available()"]
      interval: 30s
      timeout: 10s
      retries: 3

    # GPU resource allocation
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ==============================================================================
  # TRAINING SERVICE - Optimized for ML training workloads
  # ==============================================================================
  textforensics-train:
    build:
      context: .
      target: training                   # Training-optimized stage
    image: textforensics-train:${VERSION}
    container_name: textforensics-train
    extends: textforensics-dev           # Inherit from dev service

    # Training-specific environment
    environment:
      - TORCH_CUDNN_BENCHMARK=1         # Enable cuDNN benchmarking

    # Training entry point
    command: python scripts/training/train.py
    profiles: ["training"]              # Only start with --profile training

  # ==============================================================================
  # RESEARCH SERVICE - Jupyter-only environment for experimentation
  # ==============================================================================
  textforensics-research:
    build:
      context: .
      target: development                # Use development stage
    image: textforensics-dev:${VERSION}
    container_name: textforensics-research
    extends: textforensics-dev           # Inherit configuration

    # Only expose Jupyter port
    ports:
      - "${JUPYTER_PORT:-8888}:8888"

    # Jupyter-only command
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
    profiles: ["research"]              # Only start with --profile research

  # ==============================================================================
  # API SERVICE - Production API deployment
  # ==============================================================================
  textforensics-api:
    build:
      context: .
      target: production                 # Production-optimized stage
    image: textforensics-api:${VERSION}
    container_name: textforensics-api
    extends: textforensics-dev           # Inherit base configuration

    # API-specific port mapping
    ports:
      - "${API_PORT:-8000}:8000"

    # Production API server
    command: uvicorn textforensics.api:app --host 0.0.0.0 --port 8000
    profiles: ["api"]                   # Only start with --profile api

    # GPU allocation for inference
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# ==============================================================================
# PERSISTENT VOLUMES - Data persistence across container restarts
# ==============================================================================
volumes:
  textforensics-data:    # Project datasets and files
  textforensics-logs:    # Application and training logs
  textforensics-models:  # Trained model artifacts
  textforensics-cache:   # Cache for faster rebuilds
  bash-history:          # Shell command history
  vscode-server:         # VS Code server files

# Network configuration
networks:
  default:
    name: textforensics-network
